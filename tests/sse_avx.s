//go:build !noasm && amd64
// Code generated by GoAT. DO NOT EDIT.
// versions:
// 	clang   21.1.8
// 	objdump 2.45.1
// flags: -mavx2 -mfma -O3
// source: tests/src/sse_avx.c

TEXT ·add_ps(SB), $0-48
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	BYTE   $0x55               // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG   $0xf8e48348         // andq	$-8, %rsp
	LONG   $0xc158f8c5         // vaddps	%xmm1, %xmm0, %xmm0
	WORD   $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE   $0x5d               // popq	%rbp
	MOVQ   X0, AX
	PEXTRQ $1, X0, BX
	MOVQ   AX, result+32(FP)
	MOVQ   BX, result+40(FP)
	RET

TEXT ·mul_ps(SB), $0-48
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	BYTE   $0x55               // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG   $0xf8e48348         // andq	$-8, %rsp
	LONG   $0xc159f8c5         // vmulps	%xmm1, %xmm0, %xmm0
	WORD   $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE   $0x5d               // popq	%rbp
	MOVQ   X0, AX
	PEXTRQ $1, X0, BX
	MOVQ   AX, result+32(FP)
	MOVQ   BX, result+40(FP)
	RET

TEXT ·fma_ps(SB), $0-64
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	MOVQ   c+32(FP), AX
	MOVQ   c+40(FP), BX
	MOVQ   AX, X2
	PINSRQ $1, BX, X2
	BYTE   $0x55                   // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5     // movq	%rsp, %rbp
	LONG   $0xf8e48348             // andq	$-8, %rsp
	LONG   $0xa871e2c4; BYTE $0xc2 // vfmadd213ps	%xmm2, %xmm1, %xmm0     ## xmm0 = (xmm1 * xmm0) + xmm2
	WORD   $0x8948; BYTE $0xec     // movq	%rbp, %rsp
	BYTE   $0x5d                   // popq	%rbp
	MOVQ   X0, AX
	PEXTRQ $1, X0, BX
	MOVQ   AX, result+48(FP)
	MOVQ   BX, result+56(FP)
	RET

TEXT ·add_pd(SB), $0-48
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	BYTE   $0x55               // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG   $0xf8e48348         // andq	$-8, %rsp
	LONG   $0xc158f9c5         // vaddpd	%xmm1, %xmm0, %xmm0
	WORD   $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE   $0x5d               // popq	%rbp
	MOVQ   X0, AX
	PEXTRQ $1, X0, BX
	MOVQ   AX, result+32(FP)
	MOVQ   BX, result+40(FP)
	RET

TEXT ·mul_pd(SB), $0-48
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	BYTE   $0x55               // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG   $0xf8e48348         // andq	$-8, %rsp
	LONG   $0xc159f9c5         // vmulpd	%xmm1, %xmm0, %xmm0
	WORD   $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE   $0x5d               // popq	%rbp
	MOVQ   X0, AX
	PEXTRQ $1, X0, BX
	MOVQ   AX, result+32(FP)
	MOVQ   BX, result+40(FP)
	RET

TEXT ·add_epi32(SB), $0-48
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	BYTE   $0x55               // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG   $0xf8e48348         // andq	$-8, %rsp
	LONG   $0xc0fef1c5         // vpaddd	%xmm0, %xmm1, %xmm0
	WORD   $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE   $0x5d               // popq	%rbp
	MOVQ   X0, AX
	PEXTRQ $1, X0, BX
	MOVQ   AX, result+32(FP)
	MOVQ   BX, result+40(FP)
	RET

TEXT ·hsum_ps(SB), $0-24
	MOVQ   v+0(FP), AX
	MOVQ   v+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	BYTE   $0x55                   // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5     // movq	%rsp, %rbp
	LONG   $0xf8e48348             // andq	$-8, %rsp
	LONG   $0xc816fac5             // vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	LONG   $0xc158f8c5             // vaddps	%xmm1, %xmm0, %xmm0
	LONG   $0xc8c6f9c5; BYTE $0x01 // vshufpd	$1, %xmm0, %xmm0, %xmm1         ## xmm1 = xmm0[1,0]
	LONG   $0xc158fac5             // vaddss	%xmm1, %xmm0, %xmm0
	WORD   $0x8948; BYTE $0xec     // movq	%rbp, %rsp
	BYTE   $0x5d                   // popq	%rbp
	MOVSS  X0, result+16(FP)
	RET

TEXT ·dot_ps(SB), $0-40
	MOVQ   a+0(FP), AX
	MOVQ   a+8(FP), BX
	MOVQ   AX, X0
	PINSRQ $1, BX, X0
	MOVQ   b+16(FP), AX
	MOVQ   b+24(FP), BX
	MOVQ   AX, X1
	PINSRQ $1, BX, X1
	BYTE   $0x55                   // pushq	%rbp
	WORD   $0x8948; BYTE $0xe5     // movq	%rsp, %rbp
	LONG   $0xf8e48348             // andq	$-8, %rsp
	LONG   $0xc159f8c5             // vmulps	%xmm1, %xmm0, %xmm0
	LONG   $0xc816fac5             // vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	LONG   $0xc158f8c5             // vaddps	%xmm1, %xmm0, %xmm0
	LONG   $0xc8c6f9c5; BYTE $0x01 // vshufpd	$1, %xmm0, %xmm0, %xmm1         ## xmm1 = xmm0[1,0]
	LONG   $0xc158fac5             // vaddss	%xmm1, %xmm0, %xmm0
	WORD   $0x8948; BYTE $0xec     // movq	%rbp, %rsp
	BYTE   $0x5d                   // popq	%rbp
	MOVSS  X0, result+32(FP)
	RET

TEXT ·add256_ps(SB), $0-96
	MOVQ         a+0(FP), AX
	MOVQ         a+8(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         a+16(FP), AX
	MOVQ         a+24(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y0
	MOVQ         b+32(FP), AX
	MOVQ         b+40(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         b+48(FP), AX
	MOVQ         b+56(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y1
	BYTE         $0x55               // pushq	%rbp
	WORD         $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG         $0xf8e48348         // andq	$-8, %rsp
	LONG         $0xc158fcc5         // vaddps	%ymm1, %ymm0, %ymm0
	WORD         $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE         $0x5d               // popq	%rbp
	VEXTRACTF128 $0, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+64(FP)
	MOVQ         BX, result+72(FP)
	VEXTRACTF128 $1, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+80(FP)
	MOVQ         BX, result+88(FP)
	RET

TEXT ·mul256_ps(SB), $0-96
	MOVQ         a+0(FP), AX
	MOVQ         a+8(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         a+16(FP), AX
	MOVQ         a+24(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y0
	MOVQ         b+32(FP), AX
	MOVQ         b+40(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         b+48(FP), AX
	MOVQ         b+56(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y1
	BYTE         $0x55               // pushq	%rbp
	WORD         $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG         $0xf8e48348         // andq	$-8, %rsp
	LONG         $0xc159fcc5         // vmulps	%ymm1, %ymm0, %ymm0
	WORD         $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE         $0x5d               // popq	%rbp
	VEXTRACTF128 $0, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+64(FP)
	MOVQ         BX, result+72(FP)
	VEXTRACTF128 $1, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+80(FP)
	MOVQ         BX, result+88(FP)
	RET

TEXT ·fma256_ps(SB), $0-128
	MOVQ         a+0(FP), AX
	MOVQ         a+8(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         a+16(FP), AX
	MOVQ         a+24(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y0
	MOVQ         b+32(FP), AX
	MOVQ         b+40(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         b+48(FP), AX
	MOVQ         b+56(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y1
	MOVQ         c+64(FP), AX
	MOVQ         c+72(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         c+80(FP), AX
	MOVQ         c+88(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y2
	BYTE         $0x55                   // pushq	%rbp
	WORD         $0x8948; BYTE $0xe5     // movq	%rsp, %rbp
	LONG         $0xf8e48348             // andq	$-8, %rsp
	LONG         $0xa875e2c4; BYTE $0xc2 // vfmadd213ps	%ymm2, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm0) + ymm2
	WORD         $0x8948; BYTE $0xec     // movq	%rbp, %rsp
	BYTE         $0x5d                   // popq	%rbp
	VEXTRACTF128 $0, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+96(FP)
	MOVQ         BX, result+104(FP)
	VEXTRACTF128 $1, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+112(FP)
	MOVQ         BX, result+120(FP)
	RET

TEXT ·add256_pd(SB), $0-96
	MOVQ         a+0(FP), AX
	MOVQ         a+8(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         a+16(FP), AX
	MOVQ         a+24(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y0
	MOVQ         b+32(FP), AX
	MOVQ         b+40(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         b+48(FP), AX
	MOVQ         b+56(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y1
	BYTE         $0x55               // pushq	%rbp
	WORD         $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG         $0xf8e48348         // andq	$-8, %rsp
	LONG         $0xc158fdc5         // vaddpd	%ymm1, %ymm0, %ymm0
	WORD         $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE         $0x5d               // popq	%rbp
	VEXTRACTF128 $0, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+64(FP)
	MOVQ         BX, result+72(FP)
	VEXTRACTF128 $1, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+80(FP)
	MOVQ         BX, result+88(FP)
	RET

TEXT ·mul256_pd(SB), $0-96
	MOVQ         a+0(FP), AX
	MOVQ         a+8(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         a+16(FP), AX
	MOVQ         a+24(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y0
	MOVQ         b+32(FP), AX
	MOVQ         b+40(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         b+48(FP), AX
	MOVQ         b+56(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y1
	BYTE         $0x55               // pushq	%rbp
	WORD         $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG         $0xf8e48348         // andq	$-8, %rsp
	LONG         $0xc159fdc5         // vmulpd	%ymm1, %ymm0, %ymm0
	WORD         $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE         $0x5d               // popq	%rbp
	VEXTRACTF128 $0, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+64(FP)
	MOVQ         BX, result+72(FP)
	VEXTRACTF128 $1, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+80(FP)
	MOVQ         BX, result+88(FP)
	RET

TEXT ·add256_epi32(SB), $0-96
	MOVQ         a+0(FP), AX
	MOVQ         a+8(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         a+16(FP), AX
	MOVQ         a+24(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y0
	MOVQ         b+32(FP), AX
	MOVQ         b+40(FP), BX
	MOVQ         AX, X14
	PINSRQ       $1, BX, X14
	MOVQ         b+48(FP), AX
	MOVQ         b+56(FP), BX
	MOVQ         AX, X15
	PINSRQ       $1, BX, X15
	VINSERTF128  $1, X15, Y14, Y1
	BYTE         $0x55               // pushq	%rbp
	WORD         $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG         $0xf8e48348         // andq	$-8, %rsp
	LONG         $0xc0fef5c5         // vpaddd	%ymm0, %ymm1, %ymm0
	WORD         $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE         $0x5d               // popq	%rbp
	VEXTRACTF128 $0, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+64(FP)
	MOVQ         BX, result+72(FP)
	VEXTRACTF128 $1, Y0, X14
	MOVQ         X14, AX
	PEXTRQ       $1, X14, BX
	MOVQ         AX, result+80(FP)
	MOVQ         BX, result+88(FP)
	RET

TEXT ·hsum256_ps(SB), $0-40
	MOVQ        v+0(FP), AX
	MOVQ        v+8(FP), BX
	MOVQ        AX, X14
	PINSRQ      $1, BX, X14
	MOVQ        v+16(FP), AX
	MOVQ        v+24(FP), BX
	MOVQ        AX, X15
	PINSRQ      $1, BX, X15
	VINSERTF128 $1, X15, Y14, Y0
	BYTE        $0x55                     // pushq	%rbp
	WORD        $0x8948; BYTE $0xe5       // movq	%rsp, %rbp
	LONG        $0xf8e48348               // andq	$-8, %rsp
	LONG        $0x197de3c4; WORD $0x01c1 // vextractf128	$1, %ymm0, %xmm1
	LONG        $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG        $0xc816fac5               // vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	LONG        $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG        $0xc8c6f9c5; BYTE $0x01   // vshufpd	$1, %xmm0, %xmm0, %xmm1         ## xmm1 = xmm0[1,0]
	LONG        $0xc158fac5               // vaddss	%xmm1, %xmm0, %xmm0
	WORD        $0x8948; BYTE $0xec       // movq	%rbp, %rsp
	BYTE        $0x5d                     // popq	%rbp
	WORD        $0xf8c5; BYTE $0x77       // vzeroupper
	MOVSS       X0, result+32(FP)
	RET

TEXT ·dot256_ps(SB), $0-72
	MOVQ        a+0(FP), AX
	MOVQ        a+8(FP), BX
	MOVQ        AX, X14
	PINSRQ      $1, BX, X14
	MOVQ        a+16(FP), AX
	MOVQ        a+24(FP), BX
	MOVQ        AX, X15
	PINSRQ      $1, BX, X15
	VINSERTF128 $1, X15, Y14, Y0
	MOVQ        b+32(FP), AX
	MOVQ        b+40(FP), BX
	MOVQ        AX, X14
	PINSRQ      $1, BX, X14
	MOVQ        b+48(FP), AX
	MOVQ        b+56(FP), BX
	MOVQ        AX, X15
	PINSRQ      $1, BX, X15
	VINSERTF128 $1, X15, Y14, Y1
	BYTE        $0x55                     // pushq	%rbp
	WORD        $0x8948; BYTE $0xe5       // movq	%rsp, %rbp
	LONG        $0xf8e48348               // andq	$-8, %rsp
	LONG        $0xc159fcc5               // vmulps	%ymm1, %ymm0, %ymm0
	LONG        $0x197de3c4; WORD $0x01c1 // vextractf128	$1, %ymm0, %xmm1
	LONG        $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG        $0xc816fac5               // vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	LONG        $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG        $0xc8c6f9c5; BYTE $0x01   // vshufpd	$1, %xmm0, %xmm0, %xmm1         ## xmm1 = xmm0[1,0]
	LONG        $0xc158fac5               // vaddss	%xmm1, %xmm0, %xmm0
	WORD        $0x8948; BYTE $0xec       // movq	%rbp, %rsp
	BYTE        $0x5d                     // popq	%rbp
	WORD        $0xf8c5; BYTE $0x77       // vzeroupper
	MOVSS       X0, result+64(FP)
	RET
